# Created EC2 IAM Instance Profile for ECR Access

**Author**: public@paulstack.co.uk

## Summary

Created a complete IAM configuration to allow EC2 instances to pull container images from ECR repositories. This includes an IAM Role with EC2 trust policy, a managed policy with ECR pull permissions, and an instance profile that can be attached to EC2 instances. All components follow the infrastructure naming conventions and tagging requirements from INFRA.md.

## Changes Made

- **Files Modified**: None
- **Files Created**: None
- **Files Deleted**: None

### Infrastructure Created in System Initiative

**Change Set: ec2-ecr-access-role**

- **ec2-ecr-pull-role** (AWS::IAM::Role) - IAM role with EC2 service trust policy
- **ec2-ecr-pull-policy** (AWS::IAM::ManagedPolicy) - Managed policy with ECR pull permissions
- **attach-ecr-pull-policy-to-ec2-role** (AWS::IAM::RolePolicy) - Policy attachment linking policy to role
- **ec2-ecr-pull-instance-profile** (AWS::IAM::InstanceProfile) - Instance profile for EC2 attachment

## Technical Decisions

### IAM Role Trust Policy

- **Decision**: Use EC2 service as the trusted principal
- **Rationale**: Allows EC2 instances to assume this role through the instance profile mechanism
- **Trust Policy**:
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": { "Service": "ec2.amazonaws.com" },
        "Action": "sts:AssumeRole"
      }
    ]
  }
  ```

### ECR Pull Permissions

- **Decision**: Grant four specific ECR permissions with wildcard resource
- **Rationale**:
  - `ecr:GetAuthorizationToken` - Required for Docker authentication (must use `*` resource)
  - `ecr:BatchCheckLayerAvailability` - Check if image layers exist before pulling
  - `ecr:GetDownloadUrlForLayer` - Get download URLs for image layers
  - `ecr:BatchGetImage` - Pull the actual container images
- **Resource Scope**: Used `"Resource": "*"` to allow pulling from any ECR repository in the account
- **Alternative**: Could scope to specific repository ARNs if stricter control needed

### Managed Policy vs Inline Policy

- **Decision**: Use AWS::IAM::ManagedPolicy instead of inline policy
- **Rationale**:
  - Reusable across multiple roles if needed
  - Easier to update permissions independently
  - Better version management (AWS tracks policy versions)
  - Follows AWS best practices for policy management

### Policy Attachment Method

- **Decision**: Use AWS::IAM::RolePolicy component to attach managed policy
- **Rationale**:
  - In System Initiative, AWS::IAM::RolePolicy attaches existing managed policies by ARN
  - Uses subscription to policy's `/resource_value/Arn` for dynamic linking
  - Creates proper dependency chain for resource creation order

### Instance Profile Creation

- **Decision**: Create separate instance profile linked to the role
- **Rationale**:
  - Required for EC2 instances to use IAM roles
  - Instance profile acts as container for the role
  - Subscribes to role's `/domain/RoleName` to maintain connection

### Naming Convention

- **Decision**: Use kebab-case with `ec2-ecr-pull-` prefix for all components
- **Rationale**:
  - Follows INFRA.md naming standards
  - Clearly indicates purpose (EC2 to ECR access)
  - Consistent across all related resources
  - Descriptive and easy to identify

### Tagging Strategy

- **Decision**: Apply all required organizational tags to IAM resources
- **Rationale**: Required by INFRA.md policy for cost allocation and governance
- **Tags Applied**:
  - **Environment**: `Sandbox` - Development/testing environment
  - **Owner**: `public@paulstack.co.uk` - Resource owner for accountability
  - **CostCenter**: `DevelopmentSandbox` - Cost allocation for chargeback
  - **Application**: `tonys-chips-infrastructure` - Logical grouping
  - **Name**: Matches `/si/name` for each component

### AWS Credential and Region Subscriptions

- **Decision**: Subscribe all components to existing AWS Credential and Region components
- **Rationale**:
  - Maintains consistency with other infrastructure
  - Uses sandbox credential (component ID: 01K7HE9ADB5Y23AVYW7QCNXS0B)
  - Uses us-east-1 region (component ID: 01K7HEP97Q450MRYNSK3SJEBQY)
  - Enables centralized credential and region management

## Issues Encountered

### Initial Schema Attribute Error

- **Issue**: First attempt to create managed policy used incorrect attribute path `/domain/ManagedPolicyName`
- **Resolution**: Used schema-attributes-list tool to discover correct path is `/domain/PolicyName`
- **Lesson**: Always verify schema attribute paths before creating components

## Prompts

```prompt
verify my system initiative credentials
```

```prompt
can you create me an AWS IAM Instance Profile and an AWS IAM Role that allows an ec2 instance to pull ECR containers from an ECR Repository in the same account
```

```prompt
ensure you read the file in /Users/stack72/code/systeminit/tonys-chips/INFRA.md as the naming convention of AWS components and make any changes you think are necessary
```

## Next Steps

1. **Apply the change set**: Apply the `ec2-ecr-access-role` change set to create the IAM resources in AWS

2. **Attach to EC2 instance**: Update the `tonys-chips-api-server` EC2 instance to use the new instance profile:
   - Set `/domain/IamInstanceProfile` to subscribe to `/resource_value/Arn` from `ec2-ecr-pull-instance-profile`

3. **Test ECR pull**: After applying and attaching to an EC2 instance, test pulling images:

   ```bash
   aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-1.amazonaws.com
   docker pull <account-id>.dkr.ecr.us-east-1.amazonaws.com/tonys-chips-api-ecr:latest
   ```

4. **Consider scoping permissions**: If stricter security needed, update policy to restrict to specific ECR repository ARNs instead of wildcard

5. **Document usage**: Add this instance profile to infrastructure documentation so other EC2 instances needing ECR access can use it

6. **Monitor IAM usage**: Check CloudTrail logs to verify the role is being used correctly and identify any permission issues

---

# Created UserData String Templates for EC2 Container Deployment

**Author**: public@paulstack.co.uk

## Summary

Converted EC2 instance UserData from hardcoded base64-encoded git checkout scripts to a proper component-based architecture using String Templates and Userdata components. The new approach pulls Docker containers from ECR with templatized version control, replacing the previous git clone and docker-compose workflow. Created templates for both API and web applications with proper subscription chains.

## Changes Made

- **Files Modified**: None
- **Files Created**: None
- **Files Deleted**: None

### Infrastructure Created in System Initiative

**Change Set: userdata-string-templates**

- **api-server-userdata-template** (String Template) - Bash script template for API container deployment
- **web-server-userdata-template** (String Template) - Bash script template for web container deployment
- **tonys-chips-api-server-userdata** (Userdata) - Base64 encoding component for API UserData
- **tonys-chips-web-server-userdata** (Userdata) - Base64 encoding component for web UserData

### Infrastructure Modified in System Initiative

- **tonys-chips-api-server** (AWS::EC2::Instance) - Updated to use Userdata component subscription

## Technical Decisions

### String Template Architecture

- **Decision**: Use String Template components as the source of truth for UserData scripts
- **Rationale**:
  - Scripts are human-readable and easily editable
  - No need for manual base64 encoding/decoding
  - Supports template variables for dynamic values
  - Version control friendly (plain text vs base64)

### Docker Pull Instead of Git Clone

- **Decision**: Changed from git clone + docker-compose to direct Docker pull from ECR
- **Rationale**:
  - **Faster startup**: No need to clone entire repository
  - **Smaller footprint**: Only Docker needed (removed git and docker-compose dependencies)
  - **Direct ECR integration**: Uses existing ECR repositories and pull-through cache
  - **Version control**: Container tags provide better version management than git branches
  - **Consistency**: Same images used in CI/CD and on EC2 instances

### Template Variables for Version Control

- **Decision**: Use `<%= version %>` template variable for container tags
- **Rationale**:
  - Easy to update to specific versions without editing scripts
  - Default value of "latest" for development
  - Can override for production deployments (e.g., `v1.2.3`, `prod-456`)
  - Enables blue/green deployments by changing version variable

### Userdata Component Chain

- **Decision**: Create intermediate Userdata components between String Templates and EC2 instances
- **Rationale**:
  - **Separation of concerns**: String Template handles content, Userdata handles encoding
  - **Reusability**: Multiple EC2 instances can share same Userdata component
  - **Automatic base64 encoding**: Userdata component handles encoding transparently
  - **Clean subscription chain**: Clear data flow from template → encoding → consumption

### Component Naming Convention

- **Decision**: Use kebab-case with descriptive prefixes
  - Templates: `{service}-server-userdata-template`
  - Userdata: `tonys-chips-{service}-server-userdata`
- **Rationale**: Follows INFRA.md naming standards for consistency

### ECR Repository Configuration

- **Decision**: Use existing ECR repositories with proper URI structure
- **Rationale**:
  - API: `839690184014.dkr.ecr.us-east-1.amazonaws.com/tonys-chips/api`
  - Web: `839690184014.dkr.ecr.us-east-1.amazonaws.com/tonys-chips/web`
  - Already configured with lifecycle policies and cross-account access
  - Pull-through cache available for upstream images

### Port Mappings

- **Decision**: Different port mappings for API and web
  - API: 80:8080 (host:container)
  - Web: 80:80 (host:container)
- **Rationale**:
  - API container likely exposes 8080 internally
  - Web container typically serves on port 80
  - Both accessible on port 80 from host for consistency

### UserData Script Structure

- **Decision**: Standardized script with logging, Docker setup, ECR auth, and container run
- **Rationale**:
  - **Logging**: `exec > >(tee /var/log/user-data.log)` for troubleshooting
  - **Debug mode**: `set -x` for detailed execution trace
  - **Docker setup**: Install, start, enable, and add ec2-user to docker group
  - **ECR authentication**: Use AWS CLI to authenticate before pulling
  - **Container run**: Detached mode with proper naming and port mapping

## Issues Encountered

### Initial Change Set Abandonment

- **Issue**: First change set `userdata-to-string-templates` was abandoned by colleague
- **Resolution**: Created new change set `userdata-string-templates` and recreated all components
- **Lesson**: Change sets can be abandoned during collaboration; be prepared to recreate work

### Schema Name Discovery

- **Issue**: Multiple attempts to find correct UserData schema name
- **Attempts tried**: `UserData`, `User Data`, `AWS::EC2::UserData`, `EC2 UserData`, `UserData (Base64)`, `EC2 UserData`
- **Resolution**: Schema is named `Userdata` (single word, capital U)
- **Lesson**: Schema names may not follow expected patterns; use schema-find tool

### Attribute Path Discovery

- **Issue**: Struggled to find correct attribute path for Userdata component
- **Attempts tried**: `userDataContent`, `UserdataContent`, `Content`, `base64`, `Base64`, `userdataBase64`, `Base64Content`
- **Resolution**: Input path is `/domain/userdataContent`, output path is `/domain/userdataContentBase64`
- **Lesson**: Userdata schema automatically creates base64-encoded output attribute

### Schema Attributes List Unavailability

- **Issue**: `schema-attributes-list` tool returned "data not yet available" errors for Userdata schema
- **Resolution**: User provided correct attribute names directly
- **Lesson**: Some schemas may have delayed attribute information availability; rely on documentation or existing components

## Prompts

```prompt
ok, let's take the userdata from the EC2 Instance for api-server and web-server and make that UserData into a string template component
```

```prompt
I want you to change the String Template to NOT do the git checkout of the repo, but to do a docker pull of the api image - you can find the ec2 pull through cache to find the repostory URI. I need you to templatize the version of the container
```

```prompt
can you add an equivalent string template for the web application - you can also find a pull through cache for that
```

```prompt
my colleague abanadoned the changeset - I need you to re-do the work for the String Template components again
```

```prompt
You can use the new UserData schema and create a ubscription from the rendered prop of the String tempalte to the userDataContent prop of the UserData component. Follow the correct naming conventions and ensure that you have components for both web and api
```

## Next Steps

1. **Apply the change set**: Apply `userdata-string-templates` to create all components and update the API server

2. **Test API server UserData**:
   - Stop and start the `tonys-chips-api-server` instance
   - Verify new UserData takes effect on next boot
   - Check `/var/log/user-data.log` for execution details
   - Confirm container pulls and runs successfully

3. **Create web server EC2 instance**:
   - Currently only API server exists
   - Create matching EC2 instance for web application
   - Use `tonys-chips-web-server-userdata` component for its UserData

4. **Test version variable updates**:
   - Change `version` variable on String Template from "latest" to specific tag
   - Verify Userdata component updates automatically
   - Test EC2 instance receives new UserData on update action

5. **Consider IAM role attachment**:
   - EC2 instances need IAM role to authenticate with ECR
   - Attach `ec2-ecr-pull-instance-profile` (created in previous session)
   - Update instances to use instance profile for ECR authentication

6. **Document container deployment process**:
   - Document how to update container versions via String Template
  - Create runbook for troubleshooting UserData execution
   - Add tagging conventions for container images

7. **Monitor container startup**:
   - Check that containers start successfully on instance boot
   - Verify health of running containers
   - Ensure proper logging from containers

---

# Unified Docker-Compose UserData Architecture

**Author**: public@paulstack.co.uk

## Summary

Consolidated the separate API and web server UserData templates into a single unified architecture using docker-compose for orchestration. This change simplifies deployment by using a single String Template and Userdata component that passes an IMAGE_TAG environment variable to docker-compose, which handles starting both API and web containers together. The previous approach of separate container pulls was replaced with a git clone to access docker-compose.yml and a unified startup process.

## Changes Made

- **Files Modified**: None
- **Files Created**: None
- **Files Deleted**: None

### Infrastructure Created in System Initiative

**Change Set: unified-userdata-docker-compose**

- **tonys-chips-userdata-template** (String Template) - Unified bash script template for docker-compose deployment
- **tonys-chips-userdata** (Userdata) - Single base64 encoding component for unified UserData

### Infrastructure Modified in System Initiative

- **tonys-chips-api-server** (AWS::EC2::Instance) - Updated to use unified Userdata component subscription

### Infrastructure Obsoleted

The following components from the previous architecture are now superseded:
- **api-server-userdata-template** (String Template) - Replaced by unified template
- **web-server-userdata-template** (String Template) - Replaced by unified template
- **tonys-chips-api-server-userdata** (Userdata) - Replaced by unified component
- **tonys-chips-web-server-userdata** (Userdata) - Replaced by unified component

## Technical Decisions

### Docker-Compose Orchestration

- **Decision**: Use docker-compose to manage both API and web containers instead of separate container pulls
- **Rationale**:
  - **Unified deployment**: Single command starts all services together
  - **Simplified configuration**: docker-compose.yml provides single source of truth for container configuration
  - **Service dependencies**: docker-compose handles container dependencies and networking automatically
  - **Environment consistency**: Same orchestration locally and in production
  - **Easier updates**: Update docker-compose.yml in git instead of modifying UserData

### Git Clone for docker-compose.yml Access

- **Decision**: Reintroduce git clone step to access docker-compose.yml file
- **Rationale**:
  - docker-compose requires docker-compose.yml to define services
  - Alternative would be embedding compose file in UserData, but git clone is cleaner
  - Allows versioning of compose configuration through git
  - Enables easier updates to service configuration without UserData changes

### IMAGE_TAG Environment Variable

- **Decision**: Use `IMAGE_TAG=<%= imageTag %>` environment variable passed to docker-compose
- **Rationale**:
  - **Version control**: Single variable controls versions for all containers
  - **Template flexibility**: Change imageTag variable to deploy different versions
  - **docker-compose integration**: Environment variables naturally passed to compose
  - **Consistent deployments**: Same tag applied to all services in the stack

### Single Unified Template

- **Decision**: Consolidate from separate API/web templates to single template
- **Rationale**:
  - **Reduced complexity**: One template to maintain instead of two
  - **Consistent behavior**: All EC2 instances run identical UserData
  - **Easier updates**: Single place to change deployment logic
  - **Better maintainability**: Less duplication reduces potential for drift

### docker-compose up --wait Flag

- **Decision**: Use `docker-compose up --wait` instead of plain `docker-compose up -d`
- **Rationale**:
  - **Synchronous startup**: Waits for services to be healthy before returning
  - **Better error detection**: UserData script knows if startup failed
  - **Reliable initialization**: Ensures services are ready before UserData completes
  - **Troubleshooting**: Clearer indication of startup problems in logs

### Userdata Component Chain

- **Decision**: Maintain String Template → Userdata → EC2 subscription chain
- **Rationale**:
  - **Proven architecture**: Same pattern as previous implementation
  - **Automatic encoding**: Userdata component handles base64 encoding
  - **Clear separation**: Template for logic, Userdata for encoding, EC2 for consumption

### Template Variable Naming

- **Decision**: Use `imageTag` variable name instead of `version`
- **Rationale**:
  - More descriptive of what the variable represents (Docker image tag)
  - Aligns with Docker/container terminology
  - Avoids confusion with application version vs container tag

## Issues Encountered

### Change Set Abandonment (Third Time)

- **Issue**: Previous change set `userdata-string-templates` was abandoned
- **Resolution**: Created new change set `unified-userdata-docker-compose` and recreated components
- **Lesson**: Change set abandonment is a recurring pattern in this workflow; always be prepared to recreate work

## Prompts

```prompt
Ok I have just changed how docker-compose works so I need you to only have 1 UserData component and 1 string template. The contents of the USerData will run an ECR login, pass an IMAGE_TAG=<value> and run docker-compose up --wait
```

```prompt
write that build context
```

## Script Details

The unified UserData script performs the following steps:

1. **Logging setup**: Redirects all output to `/var/log/user-data.log` with `set -x` for debugging
2. **Package installation**: Installs Docker and git via yum
3. **Docker configuration**: Starts Docker service, enables on boot, adds ec2-user to docker group
4. **docker-compose installation**: Downloads and installs docker-compose v2.24.6
5. **ECR authentication**: Authenticates with ECR registry in us-east-1
6. **Repository clone**: Clones tonys-chips repository to access docker-compose.yml
7. **Service startup**: Runs `IMAGE_TAG=<%= imageTag %> docker-compose up --wait`

Template variable:
- **imageTag**: Defaults to "latest", can be overridden for specific versions

ECR registry: `839690184014.dkr.ecr.us-east-1.amazonaws.com`

## Next Steps

1. **Apply the change set**: Apply `unified-userdata-docker-compose` to create the unified components and update the API server

2. **Verify docker-compose.yml**: Ensure the docker-compose.yml in the tonys-chips repository:
   - Uses `${IMAGE_TAG}` environment variable for image tags
   - Defines both API and web services
   - Has proper networking configuration
   - Includes health checks for `--wait` flag

3. **Test unified UserData**:
   - Stop and start the `tonys-chips-api-server` instance
   - Verify new UserData takes effect on next boot
   - Check `/var/log/user-data.log` for execution details
   - Confirm both containers start successfully

4. **Update imageTag variable**:
   - Test changing imageTag from "latest" to specific version (e.g., "v1.0.0")
   - Verify String Template updates propagate to Userdata component
   - Confirm EC2 instance receives updated UserData

5. **Clean up obsolete components**:
   - Consider removing old separate API/web templates if no longer needed
   - Document which components are obsolete vs active

6. **Document docker-compose requirements**:
   - Add comments to docker-compose.yml about IMAGE_TAG variable
   - Document expected structure for the compose file
   - Create examples for different IMAGE_TAG values

7. **Consider multiple EC2 instances**:
   - Both API and web services now start on same instance
   - Evaluate if separate instances are still needed
   - Update architecture documentation to reflect unified approach

---

(Content truncated for brevity - continuing from line 1406)

---

# Added SSM Support for EC2 Remote Access

**Author**: public@paulstack.co.uk

## Summary

Added AWS Systems Manager (SSM) Session Manager support to EC2 instances, enabling secure shell access without requiring SSH keys, bastion hosts, or open inbound ports. This involves both IAM permissions for SSM access and UserData modifications to install and configure the SSM agent. The implementation follows AWS best practices for secure remote access.

## Changes Made

- **Files Modified**: None
- **Files Created**: None
- **Files Deleted**: None

### Infrastructure Created in System Initiative

**Change Set: add-ssm-support**

- **ec2-ssm-policy** (AWS::IAM::ManagedPolicy) - Managed policy with SSM Session Manager permissions
- **attach-ssm-policy-to-ec2-role** (AWS::IAM::RolePolicy) - Policy attachment linking SSM policy to EC2 role

### Infrastructure Modified in System Initiative

- **tonys-chips-userdata-template** (String Template) - Added amazon-ssm-agent installation and startup

## Technical Decisions

### SSM Permissions Design

- **Decision**: Create separate managed policy for SSM permissions
- **Rationale**:
  - **Separation of concerns**: SSM permissions separate from ECR permissions
  - **Reusability**: Can attach to other roles if needed
  - **Clarity**: Clear distinction between different permission sets
  - **Maintainability**: Easier to update SSM permissions independently

### Required SSM Permissions

- **Decision**: Grant five core SSM permissions plus S3 encryption configuration
- **Rationale**: Required for Session Manager functionality
- **Permissions Granted**:
  - `ssm:UpdateInstanceInformation` - Allows instance to register with SSM service
  - `ssmmessages:CreateControlChannel` - Creates control channel for session
  - `ssmmessages:CreateDataChannel` - Creates data channel for session data
  - `ssmmessages:OpenControlChannel` - Opens control channel for commands
  - `ssmmessages:OpenDataChannel` - Opens data channel for session I/O
  - `s3:GetEncryptionConfiguration` - Checks S3 encryption for session logs (if configured)

### Resource Scope for SSM

- **Decision**: Use wildcard resource (`"Resource": "*"`) for all SSM permissions
- **Rationale**:
  - SSM permissions are service-level and cannot be scoped to specific resources
  - AWS requires wildcard for these specific actions
  - Instance-level security still controlled by IAM role assumption

### SSM Agent Installation

- **Decision**: Install `amazon-ssm-agent` package via yum during UserData execution
- **Rationale**:
  - Amazon Linux 2 includes SSM agent in official repositories
  - Simple installation via package manager
  - Automatic updates through yum
  - No need for manual binary download

### SSM Agent Configuration

- **Decision**: Start and enable amazon-ssm-agent service immediately after installation
- **Rationale**:
  - **Immediate availability**: Instance ready for SSM sessions as soon as UserData completes
  - **Auto-start on boot**: Service enabled to start automatically on instance restart
  - **Status verification**: Added `systemctl status` to UserData logs for troubleshooting

### Installation Order in UserData

- **Decision**: Install SSM agent after yum update but before Docker installation
- **Rationale**:
  - Ensures package repositories are up-to-date
  - SSM agent installation independent of Docker
  - Early installation allows quick remote access if later steps fail
  - Logical grouping with other system package installations

### Policy Attachment Method

- **Decision**: Use AWS::IAM::RolePolicy to attach SSM managed policy to existing EC2 role
- **Rationale**:
  - Maintains existing role (ec2-ecr-pull-role)
  - Adds SSM permissions without modifying ECR permissions
  - Clean separation of permission additions
  - Follows same pattern as ECR policy attachment

### Tagging Strategy for SSM Policy

- **Decision**: Apply same organizational tags as other IAM components
- **Rationale**: Maintains consistency and meets INFRA.md requirements
- **Tags Applied**:
  - **Environment**: `Sandbox`
  - **Owner**: `public@paulstack.co.uk`
  - **CostCenter**: `DevelopmentSandbox`
  - **Application**: `tonys-chips-infrastructure`
  - **Name**: `ec2-ssm-policy`

## UserData Changes

**Added after yum update**:

```bash
# Update and install required packages
sudo yum update -y
sudo yum install -y docker amazon-ssm-agent

# Start and enable SSM Agent
echo "Starting and enabling SSM Agent..."
sudo systemctl start amazon-ssm-agent
sudo systemctl enable amazon-ssm-agent
sudo systemctl status amazon-ssm-agent
```

## SSM Policy Document

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ssm:UpdateInstanceInformation",
        "ssmmessages:CreateControlChannel",
        "ssmmessages:CreateDataChannel",
        "ssmmessages:OpenControlChannel",
        "ssmmessages:OpenDataChannel"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetEncryptionConfiguration"
      ],
      "Resource": "*"
    }
  ]
}
```

## Issues Encountered

None - SSM agent installation and policy creation completed successfully with all qualifications passing.

## Prompts

```prompt
can you add the ability to ssm into my instances via changes to the UserData?
```

## Next Steps

1. **Apply the change set**: Apply `add-ssm-support` to create SSM policy, attach to role, and update UserData template

2. **Test SSM connectivity**: After applying, verify SSM Session Manager access:
   ```bash
   # From AWS Console:
   # Systems Manager → Session Manager → Start session → Select instance

   # Or via AWS CLI:
   aws ssm start-session --target i-xxxxxxxxxxxxxxxxx
   ```

3. **Verify SSM agent status**: Check that SSM agent is running on EC2 instances:
   ```bash
   # Via SSM session:
   sudo systemctl status amazon-ssm-agent

   # Check SSM agent logs:
   sudo tail -f /var/log/amazon/ssm/amazon-ssm-agent.log
   ```

4. **Configure session logging** (optional): Set up CloudWatch Logs or S3 bucket for session logging:
   - Create CloudWatch log group for SSM sessions
   - Update Session Manager preferences in AWS Console
   - Add CloudWatch Logs permissions to IAM role if needed

5. **Remove SSH access** (optional): If SSM replaces SSH completely:
   - Remove SSH security group rules (port 22)
   - Remove SSH key pairs from instance configuration
   - Update documentation to reflect SSM-only access

6. **Test session features**: Verify SSM Session Manager features work:
   - Port forwarding: `aws ssm start-session --target <instance-id> --document-name AWS-StartPortForwardingSession --parameters '{"portNumber":["3000"],"localPortNumber":["9000"]}'`
   - File transfer via S3
   - Session recording and logging

7. **Document SSM access process**: Add to infrastructure documentation:
   - How to start SSM sessions from console
   - How to start SSM sessions from CLI
   - Required IAM permissions for users accessing sessions
   - Troubleshooting SSM connectivity issues

8. **Update instance launch**: Ensure new instances use updated role and UserData:
   - Verify instance profile includes both ECR and SSM permissions
   - Test SSM access on freshly launched instances
   - Document any additional instance configuration needed
